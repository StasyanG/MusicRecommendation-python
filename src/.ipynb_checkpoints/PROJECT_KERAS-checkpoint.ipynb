{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Merge, Activation, Dense\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from DatasetBuilder import DatasetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBuilder initialized.\n",
      "Dataset Info\n",
      "> Train set:\n",
      "> > Positive: 10000\n",
      "> > Negative: 10000\n",
      "> Test set:\n",
      "> > Positive: 1000\n",
      "> > Negative: 1000\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "\n",
    "FLDS = ['track_id', 'artist', 'title', 'similars', 'lyrics_url', 'sample_url']\n",
    "DATASET = DatasetBuilder(True,\n",
    "                         'D:\\\\PROJECT\\\\data\\\\train_data', 'D:\\\\PROJECT\\\\data\\\\train_lyrics',\n",
    "                         'D:\\\\PROJECT\\\\data\\\\test_data', 'D:\\\\PROJECT\\\\data\\\\test_lyrics',\n",
    "                         FLDS, 0.5)\n",
    "\n",
    "DATASET.load_from_file('../data/db_test.neg', False, False)\n",
    "DATASET.load_from_file('../data/db_test.pos', False, True)\n",
    "DATASET.load_from_file('../data/db_train.neg', True, False)\n",
    "DATASET.load_from_file('../data/db_train.pos', True, True)\n",
    "DATASET.info()\n",
    "\n",
    "#DATASET.read_data(False, 1000)\n",
    "#DATASET.read_data(True, 10000)\n",
    "#DATASET.dump('../data/db')\n",
    "#DATASET.info()\n",
    "\n",
    "# Join all text data into 2 lists to build vocabulary out of them\n",
    "TRAIN_DATA_1 =  [data_item['data_1'] for data_item in DATASET.train_data['pos']] + \\\n",
    "                [data_item['data_1'] for data_item in DATASET.train_data['neg']]\n",
    "TRAIN_DATA_2 =  [data_item['data_2'] for data_item in DATASET.train_data['pos']] + \\\n",
    "                [data_item['data_2'] for data_item in DATASET.train_data['neg']]\n",
    "TRAIN_SCORES =  [data_item['score'] for data_item in DATASET.train_data['pos']] + \\\n",
    "                [data_item['score'] for data_item in DATASET.train_data['neg']]\n",
    "\n",
    "TEST_DATA_1 =   [data_item['data_1'] for data_item in DATASET.test_data['pos']] + \\\n",
    "                [data_item['data_1'] for data_item in DATASET.test_data['neg']]\n",
    "TEST_DATA_2 =   [data_item['data_2'] for data_item in DATASET.test_data['pos']] + \\\n",
    "                [data_item['data_2'] for data_item in DATASET.test_data['neg']]\n",
    "TEST_SCORES =   [data_item['score'] for data_item in DATASET.test_data['pos']] + \\\n",
    "                [data_item['score'] for data_item in DATASET.test_data['neg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58403\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "word_dictionary = {}\n",
    "def unique_recursive_len(item):\n",
    "    for it in item:\n",
    "        for elem in it:\n",
    "            if elem not in word_dictionary:\n",
    "                word_dictionary[elem] = 1\n",
    "            else:\n",
    "                word_dictionary[elem] = word_dictionary[elem] + 1\n",
    "    return len(word_dictionary)\n",
    "\n",
    "def wordIsPrintable(word):\n",
    "    printable = string.printable\n",
    "    f = 1\n",
    "    for c in word:\n",
    "        if c not in printable:\n",
    "            f = 0\n",
    "            break\n",
    "    return f\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "TRAIN_DATA_1_ws = [[stemmer.stem(word) for word in text.text_to_word_sequence(t, lower=True, split=\" \")] for t in TRAIN_DATA_1]\n",
    "TRAIN_DATA_2_ws = [[stemmer.stem(word) for word in text.text_to_word_sequence(t, lower=True, split=\" \")] for t in TRAIN_DATA_2]\n",
    "TEST_DATA_1_ws = [[stemmer.stem(word) for word in text.text_to_word_sequence(t, lower=True, split=\" \")] for t in TEST_DATA_1]\n",
    "TEST_DATA_2_ws = [[stemmer.stem(word) for word in text.text_to_word_sequence(t, lower=True, split=\" \")] for t in TEST_DATA_2]\n",
    "number_of_words = unique_recursive_len(TRAIN_DATA_1_ws + TRAIN_DATA_2_ws + TEST_DATA_1_ws + TEST_DATA_2_ws)\n",
    "print(number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58403 57786\n"
     ]
    }
   ],
   "source": [
    "stemmed_dict = {}\n",
    "\n",
    "for word, count in word_dictionary.items():\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    if stemmed_word not in stemmed_dict:\n",
    "        stemmed_dict[stemmed_word] = count\n",
    "    else:\n",
    "        stemmed_dict[stemmed_word] = stemmed_dict[stemmed_word] + count\n",
    "print(len(word_dictionary), len(stemmed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_1_hot = [text.one_hot(t, VOCAB_SIZE, lower=True, split=\" \") for t in TRAIN_DATA_1]\n",
    "TRAIN_DATA_2_hot = [text.one_hot(t, VOCAB_SIZE, lower=True, split=\" \") for t in TRAIN_DATA_2]\n",
    "TEST_DATA_1_hot = [text.one_hot(t, VOCAB_SIZE, lower=True, split=\" \") for t in TEST_DATA_1]\n",
    "TEST_DATA_2_hot = [text.one_hot(t, VOCAB_SIZE, lower=True, split=\" \") for t in TEST_DATA_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "SEQUENCE_LENGTH = 200\n",
    "\n",
    "TRAIN_DATA_1_hot = sequence.pad_sequences(TRAIN_DATA_1_hot, maxlen=SEQUENCE_LENGTH)\n",
    "TRAIN_DATA_2_hot = sequence.pad_sequences(TRAIN_DATA_2_hot, maxlen=SEQUENCE_LENGTH)\n",
    "TEST_DATA_1_hot = sequence.pad_sequences(TEST_DATA_1_hot, maxlen=SEQUENCE_LENGTH)\n",
    "TEST_DATA_2_hot = sequence.pad_sequences(TEST_DATA_2_hot, maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Have to change input shape (nb_samples, timesteps, input_dim)\n",
    "TRAIN_DATA_1_hot = TRAIN_DATA_1_hot.reshape(len(TRAIN_DATA_1_hot), SEQUENCE_LENGTH, 1)\n",
    "TRAIN_DATA_2_hot = TRAIN_DATA_2_hot.reshape(len(TRAIN_DATA_2_hot), SEQUENCE_LENGTH, 1)\n",
    "TEST_DATA_1_hot = TEST_DATA_1_hot.reshape(len(TEST_DATA_1_hot), SEQUENCE_LENGTH, 1)\n",
    "TEST_DATA_2_hot = TEST_DATA_2_hot.reshape(len(TEST_DATA_2_hot), SEQUENCE_LENGTH, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_3 (LSTM)                    (None, 10)            480                                          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 10)            480                                          \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             11          merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 1)             0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 971\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 70s - loss: 0.6943 - acc: 0.5253    \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 71s - loss: 0.6881 - acc: 0.5357    \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 72s - loss: 0.6867 - acc: 0.5394    \n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 71s - loss: 0.6875 - acc: 0.5434    \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 71s - loss: 0.6916 - acc: 0.5265    \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 72s - loss: 0.6915 - acc: 0.5275    \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 72s - loss: 0.6914 - acc: 0.5268    \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 73s - loss: 0.6912 - acc: 0.5277    \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 75s - loss: 0.6913 - acc: 0.5273    \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 73s - loss: 0.6909 - acc: 0.5293    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3fdc0828>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBED_VEC_LEN = 128\n",
    "output_dim = 10\n",
    "\n",
    "first_model = Sequential()\n",
    "#first_model.add(Embedding(VOCAB_SIZE, EMBED_VEC_LEN, input_length=SEQUENCE_LENGTH))\n",
    "first_model.add(LSTM(output_dim, input_shape=(SEQUENCE_LENGTH,1)))\n",
    "\n",
    "second_model = Sequential()\n",
    "#second_model.add(Embedding(VOCAB_SIZE, EMBED_VEC_LEN, input_length=SEQUENCE_LENGTH))\n",
    "second_model.add(LSTM(output_dim, input_shape=(SEQUENCE_LENGTH,1)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge(layers=[first_model, second_model], mode='sum'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit([TRAIN_DATA_1_hot, TRAIN_DATA_2_hot], TRAIN_SCORES, nb_epoch=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s     \n",
      "Accuracy: 54.10%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate([TEST_DATA_1_hot, TEST_DATA_2_hot], TEST_SCORES, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
