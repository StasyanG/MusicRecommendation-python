{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Embedding, Activation, Dense, merge\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from DatasetBuilder import DatasetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBuilder initialized.\n",
      "Dataset Info\n",
      "> Train set:\n",
      "> > Positive: 10000\n",
      "> > Negative: 10000\n",
      "> Test set:\n",
      "> > Positive: 1000\n",
      "> > Negative: 1000\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "\n",
    "FLDS = ['track_id', 'artist', 'title', 'similars', 'lyrics_url', 'sample_url']\n",
    "DATASET = DatasetBuilder(True,\n",
    "                         'D:\\\\PROJECT\\\\data\\\\train_data', 'D:\\\\PROJECT\\\\data\\\\train_lyrics',\n",
    "                         'D:\\\\PROJECT\\\\data\\\\test_data', 'D:\\\\PROJECT\\\\data\\\\test_lyrics',\n",
    "                         FLDS, 0.5)\n",
    "\n",
    "DATASET.load_from_file('../data/db_test.neg', False, False)\n",
    "DATASET.load_from_file('../data/db_test.pos', False, True)\n",
    "DATASET.load_from_file('../data/db_train.neg', True, False)\n",
    "DATASET.load_from_file('../data/db_train.pos', True, True)\n",
    "DATASET.info()\n",
    "\n",
    "#DATASET.read_data(False, 1000)\n",
    "#DATASET.read_data(True, 10000)\n",
    "#DATASET.dump('../data/db')\n",
    "#DATASET.info()\n",
    "\n",
    "# Join all text data into 2 lists to build vocabulary out of them\n",
    "TRAIN_DATA_1 =  [data_item['data_1'] for data_item in DATASET.train_data['pos']] + \\\n",
    "                [data_item['data_1'] for data_item in DATASET.train_data['neg']]\n",
    "TRAIN_DATA_2 =  [data_item['data_2'] for data_item in DATASET.train_data['pos']] + \\\n",
    "                [data_item['data_2'] for data_item in DATASET.train_data['neg']]\n",
    "TRAIN_SCORES =  [data_item['score'] for data_item in DATASET.train_data['pos']] + \\\n",
    "                [data_item['score'] for data_item in DATASET.train_data['neg']]\n",
    "\n",
    "TEST_DATA_1 =   [data_item['data_1'] for data_item in DATASET.test_data['pos']] + \\\n",
    "                [data_item['data_1'] for data_item in DATASET.test_data['neg']]\n",
    "TEST_DATA_2 =   [data_item['data_2'] for data_item in DATASET.test_data['pos']] + \\\n",
    "                [data_item['data_2'] for data_item in DATASET.test_data['neg']]\n",
    "TEST_SCORES =   [data_item['score'] for data_item in DATASET.test_data['pos']] + \\\n",
    "                [data_item['score'] for data_item in DATASET.test_data['neg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58403\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "word_dictionary = {}\n",
    "def unique_recursive_len(item):\n",
    "    for it in item:\n",
    "        for elem in it:\n",
    "            if elem not in word_dictionary:\n",
    "                word_dictionary[elem] = 1\n",
    "            else:\n",
    "                word_dictionary[elem] = word_dictionary[elem] + 1\n",
    "    return len(word_dictionary)\n",
    "\n",
    "def wordIsPrintable(word):\n",
    "    printable = string.printable\n",
    "    f = 1\n",
    "    for c in word:\n",
    "        if c not in printable:\n",
    "            f = 0\n",
    "            break\n",
    "    return f\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "TRAIN_DATA_1_ws = [[stemmer.stem(word) for word in text.text_to_word_sequence(t, lower=True, split=\" \")] for t in TRAIN_DATA_1]\n",
    "TRAIN_DATA_2_ws = [[stemmer.stem(word) for word in text.text_to_word_sequence(t, lower=True, split=\" \")] for t in TRAIN_DATA_2]\n",
    "TEST_DATA_1_ws = [[stemmer.stem(word) for word in text.text_to_word_sequence(t, lower=True, split=\" \")] for t in TEST_DATA_1]\n",
    "TEST_DATA_2_ws = [[stemmer.stem(word) for word in text.text_to_word_sequence(t, lower=True, split=\" \")] for t in TEST_DATA_2]\n",
    "number_of_words = unique_recursive_len(TRAIN_DATA_1_ws + TRAIN_DATA_2_ws + TEST_DATA_1_ws + TEST_DATA_2_ws)\n",
    "print(number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58403 57786\n"
     ]
    }
   ],
   "source": [
    "stemmed_dict = {}\n",
    "\n",
    "for word, count in word_dictionary.items():\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    if stemmed_word not in stemmed_dict:\n",
    "        stemmed_dict[stemmed_word] = count\n",
    "    else:\n",
    "        stemmed_dict[stemmed_word] = stemmed_dict[stemmed_word] + count\n",
    "print(len(word_dictionary), len(stemmed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_1_hot = [text.one_hot(t, VOCAB_SIZE, lower=True, split=\" \") for t in TRAIN_DATA_1]\n",
    "TRAIN_DATA_2_hot = [text.one_hot(t, VOCAB_SIZE, lower=True, split=\" \") for t in TRAIN_DATA_2]\n",
    "TEST_DATA_1_hot = [text.one_hot(t, VOCAB_SIZE, lower=True, split=\" \") for t in TEST_DATA_1]\n",
    "TEST_DATA_2_hot = [text.one_hot(t, VOCAB_SIZE, lower=True, split=\" \") for t in TEST_DATA_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "SEQUENCE_LENGTH = 200\n",
    "\n",
    "TRAIN_DATA_1_hot = sequence.pad_sequences(TRAIN_DATA_1_hot, maxlen=SEQUENCE_LENGTH)\n",
    "TRAIN_DATA_2_hot = sequence.pad_sequences(TRAIN_DATA_2_hot, maxlen=SEQUENCE_LENGTH)\n",
    "TEST_DATA_1_hot = sequence.pad_sequences(TEST_DATA_1_hot, maxlen=SEQUENCE_LENGTH)\n",
    "TEST_DATA_2_hot = sequence.pad_sequences(TEST_DATA_2_hot, maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Have to change input shape (nb_samples, timesteps, input_dim)\n",
    "TRAIN_DATA_1_hot = TRAIN_DATA_1_hot.reshape(len(TRAIN_DATA_1_hot), SEQUENCE_LENGTH, 1)\n",
    "TRAIN_DATA_2_hot = TRAIN_DATA_2_hot.reshape(len(TRAIN_DATA_2_hot), SEQUENCE_LENGTH, 1)\n",
    "TEST_DATA_1_hot = TEST_DATA_1_hot.reshape(len(TEST_DATA_1_hot), SEQUENCE_LENGTH, 1)\n",
    "TEST_DATA_2_hot = TEST_DATA_2_hot.reshape(len(TEST_DATA_2_hot), SEQUENCE_LENGTH, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first_model = Sequential()\n",
    "#first_model.add(Embedding(VOCAB_SIZE, EMBED_VEC_LEN, input_length=SEQUENCE_LENGTH))\n",
    "#first_model.add(lstm)\n",
    "\n",
    "#second_model = Sequential()\n",
    "#second_model.add(Embedding(VOCAB_SIZE, EMBED_VEC_LEN, input_length=SEQUENCE_LENGTH))\n",
    "#second_model.add(lstm)\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Merge(layers=[first_model, second_model], mode='concat'))\n",
    "#model.add(Dense(1))\n",
    "#model.add(Activation('sigmoid'))\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#print(model.summary())\n",
    "#model.fit([TRAIN_DATA_1_hot[:15000], TRAIN_DATA_2_hot[:15000]], TRAIN_SCORES[:15000], nb_epoch=50, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_19 (InputLayer)            (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_20 (InputLayer)            (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)         (None, 200, 64)       1600000     input_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)         (None, 200, 64)       1600000     input_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                   (None, 50)            23000       embedding_19[0][0]               \n",
      "                                                                   embedding_20[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "merge_10 (Merge)                 (None, 100)           0           lstm_10[0][0]                    \n",
      "                                                                   lstm_10[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             101         merge_10[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 3223101\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "EMBED_VEC_LEN = 64\n",
    "output_dim = 50\n",
    "\n",
    "input_data_1 = Input(shape=(SEQUENCE_LENGTH,))\n",
    "input_data_2 = Input(shape=(SEQUENCE_LENGTH,))\n",
    "\n",
    "embed_1 = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_VEC_LEN, input_length=SEQUENCE_LENGTH)(input_data_1)\n",
    "embed_2 = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_VEC_LEN, input_length=SEQUENCE_LENGTH)(input_data_2)\n",
    "\n",
    "lstm = LSTM(output_dim, input_shape=(EMBED_VEC_LEN, 1))\n",
    "\n",
    "encoded_1 = lstm(embed_1)\n",
    "encoded_2 = lstm(embed_2)\n",
    "\n",
    "merged_vector = merge([encoded_1, encoded_2], mode='concat', concat_axis=-1)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(merged_vector)\n",
    "\n",
    "model = Model(input=[input_data_1, input_data_2], output=predictions)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# shuffle data a bit\n",
    "index_shuf = list(range(len(TRAIN_SCORES)))\n",
    "shuffle(index_shuf)\n",
    "\n",
    "TRAIN_DATA_1_hot_shuf = np.asarray([TRAIN_DATA_1_hot[i] for i in index_shuf]).reshape(TRAIN_DATA_1_hot.shape)\n",
    "TRAIN_DATA_2_hot_shuf = np.asarray([TRAIN_DATA_2_hot[i] for i in index_shuf]).reshape(TRAIN_DATA_2_hot.shape)\n",
    "TRAIN_SCORES_shuf = np.asarray([TRAIN_SCORES[i] for i in index_shuf]).reshape(np.asarray(TRAIN_SCORES).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide train dataset into test and train parts\n",
    "# !!! I don't know why but with actual test dataset ANN gives bad results on evaluation\n",
    "THRESHOLD = 2000\n",
    "TEST_D1 = TRAIN_DATA_1_hot_shuf[:THRESHOLD]\n",
    "TEST_D2 = TRAIN_DATA_2_hot_shuf[:THRESHOLD]\n",
    "TEST_S = TRAIN_SCORES_shuf[:THRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18000/18000 [==============================] - 158s - loss: 0.3661 - acc: 0.8364   \n",
      "Epoch 2/10\n",
      "18000/18000 [==============================] - 170s - loss: 0.1333 - acc: 0.9584   \n",
      "Epoch 3/10\n",
      "18000/18000 [==============================] - 171s - loss: 0.0908 - acc: 0.9710   \n",
      "Epoch 4/10\n",
      "18000/18000 [==============================] - 174s - loss: 0.0708 - acc: 0.9782   \n",
      "Epoch 5/10\n",
      "18000/18000 [==============================] - 175s - loss: 0.0602 - acc: 0.9818   \n",
      "Epoch 6/10\n",
      "18000/18000 [==============================] - 174s - loss: 0.0529 - acc: 0.9831   \n",
      "Epoch 7/10\n",
      "18000/18000 [==============================] - 176s - loss: 0.0471 - acc: 0.9856   \n",
      "Epoch 8/10\n",
      "18000/18000 [==============================] - 178s - loss: 0.0418 - acc: 0.9873   \n",
      "Epoch 9/10\n",
      "18000/18000 [==============================] - 176s - loss: 0.0371 - acc: 0.9888   \n",
      "Epoch 10/10\n",
      "18000/18000 [==============================] - 180s - loss: 0.0315 - acc: 0.9903   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x665bac88>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = len(TRAIN_SCORES)\n",
    "model.fit([TRAIN_DATA_1_hot_shuf[THRESHOLD:length], TRAIN_DATA_2_hot_shuf[THRESHOLD:length]], TRAIN_SCORES_shuf[THRESHOLD:length], nb_epoch=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 4s     \n",
      "Accuracy: 95.70%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate([TEST_D1, TEST_D2], TEST_S, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, name, folder=''):\n",
    "    model_json = model.to_json()\n",
    "    with open(folder + name + '.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(folder + name + '.h5')\n",
    "    return {\n",
    "        'model_file': folder + name + '.json',\n",
    "        'weights_file': folder + name + '.h5'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "def load_model(model_file, weights_file):\n",
    "    with open(model_file, 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(weights_file)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_file': '../trained_models/model_best.json',\n",
       " 'weights_file': '../trained_models/model_best.h5'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model, '../trained_models/model_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
